{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4160e264bc3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2\n",
    "from scipy import ndimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_resize = [28,28]\n",
    "epoch_num = None   # dataset.repeat() 的参数，设置为None，可以不断取数\n",
    "# 传入图片名，返回正则化后的图片的像素值\n",
    "def read_img(img_name, lable):\n",
    "    image = tf.read_file(img_name)\n",
    "    image = tf.image.decode_jpeg(image,channels=3)\n",
    "    image = cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2GRAY)\n",
    "    image=image/255\n",
    "\n",
    "    image=torch.from_numpy(image)\n",
    "    image= tf.cast(image, tf.float32)\n",
    "    return image,lable\n",
    "    return image,lable\n",
    "# 传入图片所在的文件夹，图片名含有图片的lable，返回利用文件夹中图片创建的dataset\n",
    "def create_dataset(path):\n",
    "    files = os.listdir(path)  # 列出文件夹中所有的图片\n",
    "    img_names = []\n",
    "    lables = []\n",
    "    for f in files:\n",
    "        img_names.append(os.path.join(path,f))   # 图片的完整路径append到文件名list中\n",
    "        order = int(f.split('.')[0])\n",
    "\n",
    "        lables.append(d[1][order])  # 根据规则得到图片的lable\n",
    "\n",
    "    img_names = tf.convert_to_tensor(img_names, dtype=tf.string)\n",
    "    lables = tf.convert_to_tensor(lables, dtype=tf.int64)  # 将图片名list和lable的list转换成Tensor类型\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_names,lables))  # 创建dataset，传入的需要是tensor类型\n",
    "    dataset = dataset.map(read_img)   # 传入read_img函数，将图片名转为像素　　 　　# 将dataset打乱，设置一次获取batch_size条数据 \n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = pd.read_csv('E:/21Cam/data/lungcancer_image/valid.csv',encoding='utf-8',header=None,index_col=0)\n",
    "gray_valid_set_raw = create_dataset('E:/21Cam/data/lungcancer_image/ajust_image/gray/valid')   # 图片所在的路径为./img\n",
    "iterator = gray_valid_set_raw.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()  # 创建dataset是batch_size 为多少这里一次就能获取多少个数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_valid_set_raw\n",
    "color_test_set_raw \n",
    "gray_train_set_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "dataset_size=555\n",
    "class_names=['POORLY DIFFERENTIATED','Moderate Differentiation','WELL DIFFERENTIATED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 22\n",
    "gray_train_set = gray_train_set_raw.shuffle(500).repeat().batch(batch_size).prefetch(1)\n",
    "gray_valid_set = gray_valid_set_raw.batch(batch_size).prefetch(1)\n",
    "gray_test_set = gray_test_set_raw.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "index = 0\n",
    "\n",
    "for image, label in gray_valid_set_raw.take(4):\n",
    "\n",
    "    index += 1\n",
    "\n",
    "    plt.subplot(2, 2, index)\n",
    "    plt.imshow(image[0])\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in gray_train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "    \n",
    "        plt.imshow(X_batch[index])\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model1 = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "avg1 = keras.layers.GlobalAveragePooling2D()(base_model1.output)\n",
    "output1 = keras.layers.Dense(n_classes, activation=\"softmax\")(avg1)\n",
    "model1 = keras.models.Model(inputs=base_model1.input, outputs=output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model1.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer1 = keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9,nesterov=True, decay=1e-4)\n",
    "model1.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer1,metrics=[\"accuracy\"])\n",
    "history1 = model1.fit(gray_train_set,steps_per_epoch=int(391 / batch_size),\n",
    "                    validation_data=gray_valid_set,\n",
    "                    validation_steps=int(107 / batch_size),\n",
    "                    epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = model1.evaluate(gray_test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
